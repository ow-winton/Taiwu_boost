import os
import json
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup

# åŠ è½½é¡µé¢é“¾æ¥
with open("taiwu_wiki_links.json", "r", encoding="utf-8") as f:
    wiki_links = json.load(f)

# å‡†å¤‡å·²æŠ“å–è¿‡çš„é“¾æ¥ï¼ˆé¿å…é‡å¤ï¼‰
output_file = "taiwu_wiki_pages.jsonl"
fetched_urls = set()
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                fetched_urls.add(obj["url"])
            except:
                pass  # é˜²æ­¢æŸè¡Œè§£æå¤±è´¥

# å¯åŠ¨ Selenium
service = Service(r'C:\Users\21293\Desktop\taiwu_exe\chromedriver-win64\chromedriver.exe')
chrome_options = Options()
# chrome_options.add_argument("--headless")  # ç”Ÿäº§æ¨¡å¼ä¸‹æ¨èå¼€å¯
driver = webdriver.Chrome(service=service, options=chrome_options)

with open(output_file, "a", encoding="utf-8") as f_out:
    for i, url in enumerate(wiki_links):
        if url in fetched_urls:
            print(f"âœ… å·²æŠ“å–è¿‡ï¼Œè·³è¿‡ï¼š{url}")
            continue

        try:
            print(f"ğŸ“˜ æŠ“å–ç¬¬ {i+1}/{len(wiki_links)} é¡µï¼š{url}")
            driver.get(url)
            time.sleep(3)

            soup = BeautifulSoup(driver.page_source, "html.parser")
            content_div = soup.find("div", class_="mw-parser-output")
            if content_div:
                text = content_div.get_text(separator="\n", strip=True)
                record = {"url": url, "text": text}
                json.dump(record, f_out, ensure_ascii=False)
                f_out.write("\n")
            else:
                print("âš ï¸ æ²¡æ‰¾åˆ°é¡µé¢å†…å®¹ï¼Œè·³è¿‡")
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ï¼š{url}\né”™è¯¯ä¿¡æ¯ï¼š{e}")
            continue

driver.quit()
print(f"\nâœ… æ‰€æœ‰é¡µé¢å¤„ç†å®Œæ¯•ï¼Œç»“æœä¿å­˜åˆ° {output_file}")
